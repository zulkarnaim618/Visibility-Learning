{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "class SceneDataset(Dataset):\n",
    "    def __init__(self, file_path, do_augmentation=True):\n",
    "        self.file_path = file_path\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.cache = []  # To store parsed scenes\n",
    "        self.samples = []  # To store indices of individual samples\n",
    "\n",
    "        # Read and parse the file during initialization\n",
    "        self._parse_file()\n",
    "\n",
    "    def _parse_file(self):\n",
    "        \"\"\"Reads the file and caches scenes and their queries.\"\"\"\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            for line_idx, line in enumerate(file):\n",
    "                line = line.strip().split()\n",
    "\n",
    "                # Parse obstacles\n",
    "                obstacles = []\n",
    "                idx = 0\n",
    "                while idx < len(line) and line[idx] == 'p':\n",
    "                    idx += 1  # Skip 'p'\n",
    "                    obstacle = []\n",
    "                    while idx < len(line) and line[idx] != 'p' and line[idx] != 'q':\n",
    "                        x, y = float(line[idx]), float(line[idx + 1])\n",
    "                        obstacle.append((x, y))\n",
    "                        idx += 2\n",
    "                    obstacles.append(obstacle)\n",
    "\n",
    "                # Parse queries and labels\n",
    "                queries = []\n",
    "                while idx < len(line):\n",
    "                    if line[idx] == 'q':\n",
    "                        idx += 1  # Skip 'q'\n",
    "                        query = []\n",
    "                        for _ in range(2):  # Each query has 2 coordinate pairs\n",
    "                            x, y = float(line[idx]), float(line[idx + 1])\n",
    "                            query.append((x, y))\n",
    "                            idx += 2\n",
    "                        label = int(line[idx])  # Label follows the query\n",
    "                        idx += 1\n",
    "                        queries.append((query, label))\n",
    "\n",
    "                # Cache the parsed scene\n",
    "                scene_idx = len(self.cache)\n",
    "                self.cache.append((obstacles, queries))\n",
    "\n",
    "                # Index individual samples\n",
    "                for query_idx in range(len(queries)):\n",
    "                    self.samples.append((scene_idx, query_idx))\n",
    "\n",
    "    def _generate_vertex_order(self, vertices):\n",
    "        \"\"\"Generates cyclic random orders for vertices.\"\"\"\n",
    "        n = len(vertices)\n",
    "        orders = []\n",
    "        r = 1\n",
    "        if self.do_augmentation:\n",
    "            r = 2\n",
    "        for _ in range(r):\n",
    "            start_idx = random.randint(0, n - 1)\n",
    "            orders.append(list(range(start_idx, n)) + list(range(0, start_idx)))\n",
    "        return orders\n",
    "\n",
    "    def _generate_obstacle_order(self, num_obstacles, num_order):\n",
    "        \"\"\"Generates random orders for obstacles.\"\"\"\n",
    "        orders = []\n",
    "        if not self.do_augmentation:\n",
    "            num_order = 1\n",
    "        for _ in range(num_order):\n",
    "            orders.append(random.sample(range(num_obstacles), num_obstacles))\n",
    "        return orders\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples (queries).\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a single sample: obstacles, query, label, and augmentation orders.\"\"\"\n",
    "        scene_idx, query_idx = self.samples[idx]\n",
    "        obstacles, queries = self.cache[scene_idx]\n",
    "        query, label = queries[query_idx]\n",
    "\n",
    "        # Generate augmentation orders\n",
    "        vertex_orders = [self._generate_vertex_order(obstacle) for obstacle in obstacles]\n",
    "        obstacle_order = self._generate_obstacle_order(len(obstacles),3)\n",
    "\n",
    "        return {\n",
    "            'obstacles': obstacles,        # Original obstacle coordinates\n",
    "            'query': query,                # Original query coordinates\n",
    "            'label': label,                # Binary label (0 or 1)\n",
    "            'vertex_orders': vertex_orders, # Vertex augmentation orders per obstacle\n",
    "            'obstacle_order': obstacle_order # Obstacle augmentation orders\n",
    "        }\n",
    "\n",
    "# Custom collate function for batching\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Prepares a batch by grouping obstacles, queries, labels, and orders.\"\"\"\n",
    "    obstacle_batch = []\n",
    "    query_batch = []\n",
    "    label_batch = []\n",
    "    vertex_orders_batch = []\n",
    "    obstacle_orders_batch = []\n",
    "\n",
    "    for item in batch:\n",
    "        obstacle_batch.append(item['obstacles'])\n",
    "        query_batch.append(item['query'])\n",
    "        label_batch.append(item['label'])\n",
    "        vertex_orders_batch.append(item['vertex_orders'])\n",
    "        obstacle_orders_batch.append(item['obstacle_order'])\n",
    "\n",
    "    return {\n",
    "        'obstacles': obstacle_batch,          # List of obstacles for each sample\n",
    "        'queries': query_batch,               # List of queries for each sample\n",
    "        'labels': torch.tensor(label_batch, dtype=torch.float),  # Labels as tensor\n",
    "        'vertex_orders': vertex_orders_batch, # Vertex augmentation orders\n",
    "        'obstacle_orders': obstacle_orders_batch # Obstacle augmentation orders\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = SceneDataset(\"/kaggle/input/data0007/train_3500.txt\")\n",
    "val_dataset = SceneDataset(\"/kaggle/input/data0001/val_50.txt\",do_augmentation=False)\n",
    "test_dataset = SceneDataset(\"/kaggle/input/data0010/train_6000.txt\",do_augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the model\n",
    "class SceneQueryModel(nn.Module):\n",
    "    def __init__(self, vertex_input_dim, obstacle_hidden_dim, scene_hidden_dim, output_dim):\n",
    "        super(SceneQueryModel, self).__init__()\n",
    "\n",
    "        # Shared RNN block for obstacles and queries\n",
    "        self.rnn_obstacle = nn.LSTM(input_size=vertex_input_dim, hidden_size=obstacle_hidden_dim, \n",
    "                                    num_layers=1, batch_first=True, dropout=0.2)\n",
    "\n",
    "        self.obstacle_embedding_fc = nn.Sequential(\n",
    "            nn.Linear(obstacle_hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "\n",
    "        # Scene-level RNN block\n",
    "        self.rnn_scene = nn.LSTM(input_size=32, hidden_size=scene_hidden_dim, \n",
    "                                 num_layers=1, batch_first=True, dropout=0.2)\n",
    "\n",
    "        self.scene_embedding_fc = nn.Sequential(\n",
    "            nn.Linear(scene_hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "\n",
    "        # Final classification block\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.Linear(8, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, obstacles, queries, vertex_orders, obstacle_orders):\n",
    "        device = next(self.parameters()).device  # Automatically get the device of the model\n",
    "    \n",
    "        # Flatten obstacles and vertex_orders for batch processing\n",
    "        flat_obstacles = []\n",
    "        flat_orders = []\n",
    "        batch_indices = []\n",
    "        obstacle_indices = []\n",
    "        flat_obstacle_indices = []\n",
    "        order_lengths = []\n",
    "    \n",
    "        m = 0\n",
    "        for i, (obstacle_set, vertex_order_set) in enumerate(zip(obstacles, vertex_orders)):\n",
    "            for j, (obstacle, orders) in enumerate(zip(obstacle_set, vertex_order_set)):\n",
    "                flat_obstacles.append(torch.tensor(obstacle, dtype=torch.float, device=device))\n",
    "                for order in orders:\n",
    "                    flat_obstacle_indices.append(m)\n",
    "                    flat_orders.append(order)\n",
    "                    batch_indices.append(i)\n",
    "                    obstacle_indices.append(j)\n",
    "                    order_lengths.append(len(order))\n",
    "                m += 1\n",
    "    \n",
    "        # Pad vertex orders\n",
    "        max_order_length = max(order_lengths)\n",
    "        padded_orders = torch.zeros((len(flat_orders), max_order_length), dtype=torch.long, device=device)\n",
    "        for idx, order in enumerate(flat_orders):\n",
    "            padded_orders[idx, :len(order)] = torch.tensor(order, dtype=torch.long, device=device)\n",
    "    \n",
    "        # Pad obstacles\n",
    "        max_vertices = max(len(obs) for obs in flat_obstacles)\n",
    "        padded_obstacles = torch.zeros((len(flat_obstacles), max_vertices, 2), dtype=torch.float, device=device)\n",
    "        for idx, obs in enumerate(flat_obstacles):\n",
    "            padded_obstacles[idx, :len(obs)] = obs\n",
    "    \n",
    "        # Reorder vertices according to padded orders\n",
    "        ordered_vertices = []\n",
    "        for k in range(len(padded_orders)):\n",
    "            order = padded_orders[k]\n",
    "            vertices = padded_obstacles[flat_obstacle_indices[k]]\n",
    "            ordered_vertices.append(vertices[order])\n",
    "    \n",
    "        ordered_vertices = torch.stack(ordered_vertices)\n",
    "    \n",
    "        # Create sequence lengths for packing\n",
    "        sequence_lengths = torch.tensor([len(order) for order in flat_orders], device=device)\n",
    "        # Move sequence_lengths to the CPU and convert to int64 for compatibility\n",
    "        sequence_lengths = sequence_lengths.cpu().to(torch.int64)\n",
    "    \n",
    "        # Pack the sequences for RNN\n",
    "        packed_vertices = nn.utils.rnn.pack_padded_sequence(ordered_vertices, sequence_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.rnn_obstacle(packed_vertices)\n",
    "    \n",
    "        # Compute embeddings for each order\n",
    "        embeddings = self.obstacle_embedding_fc(h_n[-1])  # Shape: (total_orders, embedding_output_size)\n",
    "    \n",
    "        # Aggregate embeddings back to obstacle level\n",
    "        obstacle_embeddings = torch.zeros((len(obstacles), max([len(o) for o in obstacles]), embeddings.size(-1)), device=device)\n",
    "        order_counts = torch.zeros_like(obstacle_embeddings[..., 0])  # For averaging\n",
    "    \n",
    "        for i, (batch_idx, obstacle_idx) in enumerate(zip(batch_indices, obstacle_indices)):\n",
    "            obstacle_embeddings[batch_idx, obstacle_idx] += embeddings[i]\n",
    "            order_counts[batch_idx, obstacle_idx] += 1\n",
    "    \n",
    "        # Avoid division by zero and compute the mean\n",
    "        obstacle_embeddings /= order_counts.unsqueeze(-1).clamp(min=1)\n",
    "    \n",
    "        flat_orders = []\n",
    "        batch_indices = []\n",
    "        order_lengths = []\n",
    "    \n",
    "        for i, orders in enumerate(obstacle_orders):\n",
    "            for order in orders:\n",
    "                flat_orders.append(order)\n",
    "                batch_indices.append(i)\n",
    "                order_lengths.append(len(order))\n",
    "    \n",
    "        max_order_length = max(order_lengths)\n",
    "        padded_orders = torch.zeros((len(flat_orders), max_order_length), dtype=torch.long, device=device)\n",
    "        for idx, order in enumerate(flat_orders):\n",
    "            padded_orders[idx, :len(order)] = torch.tensor(order, dtype=torch.long, device=device)\n",
    "    \n",
    "        ordered_obstacles = []\n",
    "        for k in range(len(padded_orders)):\n",
    "            order = padded_orders[k]\n",
    "            embed = obstacle_embeddings[batch_indices[k]]\n",
    "            ordered_obstacles.append(embed[order])\n",
    "    \n",
    "        ordered_obstacles = torch.stack(ordered_obstacles)\n",
    "    \n",
    "        # Create sequence lengths for packing\n",
    "        sequence_lengths = torch.tensor([len(order) for order in flat_orders], device=device)\n",
    "        # Move sequence_lengths to the CPU and convert to int64 for compatibility\n",
    "        sequence_lengths = sequence_lengths.cpu().to(torch.int64)\n",
    "    \n",
    "        # Pack the sequences for RNN\n",
    "        packed_obstacles = nn.utils.rnn.pack_padded_sequence(ordered_obstacles, sequence_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.rnn_scene(packed_obstacles)\n",
    "    \n",
    "        # Compute embeddings for each order\n",
    "        flat_scene_embeddings = self.scene_embedding_fc(h_n[-1])   # Shape: (total_orders, embedding_output_size)\n",
    "    \n",
    "        # Aggregate embeddings back to obstacle level\n",
    "        scene_embeddings = torch.zeros((len(obstacles), flat_scene_embeddings.size(-1)), device=device)\n",
    "        order_counts = torch.zeros_like(scene_embeddings[..., 0])  # For averaging\n",
    "    \n",
    "        for i, batch_idx in enumerate(batch_indices):\n",
    "            scene_embeddings[batch_idx] += flat_scene_embeddings[i]\n",
    "            order_counts[batch_idx] += 1\n",
    "    \n",
    "        # Avoid division by zero and compute the mean\n",
    "        scene_embeddings /= order_counts.unsqueeze(-1).clamp(min=1)\n",
    "    \n",
    "        # Convert queries into a tensor\n",
    "        queries_tensor = torch.tensor(queries, dtype=torch.float, device=device)  # Shape: [batch_size, seq_len, feature_dim]\n",
    "    \n",
    "        # Pass the batch through the RNN\n",
    "        _, (h_n, _) = self.rnn_obstacle(queries_tensor)  # h_n shape: [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        # Use the last layer's hidden state (for standard RNN or GRU, use h_n[-1]; for LSTM, use the hidden state only)\n",
    "        query_embeddings = self.obstacle_embedding_fc(h_n[-1])  # Shape: [batch_size, embedding_dim]\n",
    "    \n",
    "        # Concatenate query and scene embeddings\n",
    "        combined = torch.cat((query_embeddings, scene_embeddings), dim=1)\n",
    "    \n",
    "        # Classification\n",
    "        outputs = self.classifier(combined)\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "\n",
    "def calculate_metrics(outputs, labels):\n",
    "    # Convert probabilities to binary predictions\n",
    "    predictions = (outputs > 0.5).float()\n",
    "\n",
    "    # Flatten the tensors\n",
    "    predictions = predictions.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.cpu(), predictions.cpu(), labels=[0, 1]).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels.cpu(), predictions.cpu())\n",
    "    precision = precision_score(labels.cpu(), predictions.cpu(), zero_division=0)\n",
    "    recall = recall_score(labels.cpu(), predictions.cpu(), zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = f1_score(labels.cpu(), predictions.cpu(), zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, specificity, f1\n",
    "\n",
    "def train(model, train_loader, val_loader, val1_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Check if a checkpoint exists and load it\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path_load):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path_load, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Use tqdm for batch-level progress tracking\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\") as batch_bar:\n",
    "            for batch in batch_bar:\n",
    "                obstacles = batch['obstacles']\n",
    "                queries = batch['queries']\n",
    "                labels = batch['labels'].to(device)\n",
    "                vertex_orders = batch['vertex_orders']\n",
    "                obstacle_orders = batch['obstacle_orders']\n",
    "        \n",
    "                labels = labels.unsqueeze(1)  # For BCEWithLogitsLoss\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(obstacles, queries, vertex_orders, obstacle_orders)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                train_loss += loss.item()\n",
    "        \n",
    "                # Move outputs and labels to CPU to save GPU memory and store them\n",
    "                all_outputs.append(outputs.detach().cpu())\n",
    "                all_labels.append(labels.detach().cpu())\n",
    "        \n",
    "                # Concatenate all outputs and labels up to the current batch for metric calculation\n",
    "                cumulative_outputs = torch.cat(all_outputs)\n",
    "                cumulative_labels = torch.cat(all_labels)\n",
    "        \n",
    "                # Calculate cumulative metrics using the provided function\n",
    "                train_accuracy, train_precision, train_recall, train_specificity, train_f1 = calculate_metrics(cumulative_outputs, cumulative_labels)\n",
    "        \n",
    "                # Update progress bar with running loss and metrics\n",
    "                batch_bar.set_postfix(\n",
    "                    loss=train_loss / (batch_bar.n + 1),\n",
    "                    accuracy=train_accuracy,\n",
    "                    precision=train_precision,\n",
    "                    recall=train_recall,\n",
    "                    specificity=train_specificity,\n",
    "                    f1=train_f1\n",
    "                )\n",
    "        \n",
    "        # Average the training loss\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Print training metrics for the epoch\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Precision: {train_precision:.4f}, \"\n",
    "              f\"Train Recall: {train_recall:.4f}, Train Specificity: {train_specificity:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                obstacles = batch['obstacles']\n",
    "                queries = batch['queries']\n",
    "                labels = batch['labels'].to(device)\n",
    "                vertex_orders = batch['vertex_orders']\n",
    "                obstacle_orders = batch['obstacle_orders']\n",
    "        \n",
    "                labels = labels.unsqueeze(1)\n",
    "        \n",
    "                outputs = model(obstacles, queries, vertex_orders, obstacle_orders)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "                # Store outputs and labels for metric calculation\n",
    "                all_outputs.append(outputs.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        # Concatenate all outputs and labels\n",
    "        all_outputs = torch.cat(all_outputs)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        # Compute metrics using the provided function\n",
    "        val_accuracy, val_precision, val_recall, val_specificity, val_f1 = calculate_metrics(all_outputs, all_labels)\n",
    "        \n",
    "        # Average the validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Print validation metrics for the epoch\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "              f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, \"\n",
    "              f\"Validation Specificity: {val_specificity:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save checkpoint every 1 epochs\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path_save)\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "def test(model, test_loader):\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Check if a checkpoint exists and load it\n",
    "    if os.path.exists(checkpoint_path_load):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path_load, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            obstacles = batch['obstacles']\n",
    "            queries = batch['queries']\n",
    "            labels = batch['labels'].to(device)\n",
    "            vertex_orders = batch['vertex_orders']\n",
    "            obstacle_orders = batch['obstacle_orders']\n",
    "    \n",
    "            labels = labels.unsqueeze(1)\n",
    "    \n",
    "            outputs = model(obstacles, queries, vertex_orders, obstacle_orders)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            # Store outputs and labels for metric calculation\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all outputs and labels\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Compute metrics using the provided function\n",
    "    test_accuracy, test_precision, test_recall, test_specificity, test_f1 = calculate_metrics(all_outputs, all_labels)\n",
    "    \n",
    "    # Average the test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "          f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, \"\n",
    "          f\"Test Specificity: {test_specificity:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Model instantiation and dataloader setup\n",
    "if __name__ == \"__main__\":\n",
    "    vertex_input_dim = 2\n",
    "    obstacle_hidden_dim = 128\n",
    "    scene_hidden_dim = 512\n",
    "    output_dim = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SceneQueryModel(vertex_input_dim, obstacle_hidden_dim, scene_hidden_dim, output_dim).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    checkpoint_path_save ='checkpoint.pth'\n",
    "    checkpoint_path_load = '/kaggle/input/model12/pytorch/default/1/checkpoint (7).pth'\n",
    "\n",
    "    train(model, train_loader, val_loader, num_epochs=15, learning_rate=0.001)\n",
    "    \n",
    "    test(model,test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
