{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from disk 34\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 23\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 33\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 1\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 29\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 46\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 41\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 38\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 39\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 27\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 14\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 30\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 21\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 45\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 31\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 10\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 48\n",
      "Batch Data Shape: (848, 32, 32, 2)\n",
      "Batch Labels Shape: (848,)\n",
      "reading from disk 28\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 37\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 36\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 3\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 12\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 47\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 35\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 4\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 16\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 42\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 13\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 15\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 44\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 32\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 9\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 22\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 40\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 7\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 19\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 5\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 18\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 2\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 11\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 17\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 24\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 26\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 25\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 0\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 8\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 6\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 20\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n",
      "reading from disk 43\n",
      "Batch Data Shape: (1024, 32, 32, 2)\n",
      "Batch Labels Shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class TwoChannelVisibilityDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, points_file, encodings_file, points_per_scene, resolution, batch_size=32, caching=True, cache_factor=1):\n",
    "        self.points_file = points_file\n",
    "        self.encodings_file = encodings_file\n",
    "        self.batch_size = batch_size\n",
    "        self.points_per_scene = points_per_scene\n",
    "        self.resolution = resolution\n",
    "        self.caching = caching\n",
    "        self.cache_factor = cache_factor\n",
    "        self.indices = None\n",
    "        self._load_indices()\n",
    "        self.cache = [False for i in range(int(self.__len__()*self.cache_factor))]\n",
    "\n",
    "    def _load_indices(self):\n",
    "        with open(self.points_file, 'r') as f:\n",
    "            total_points = sum(1 for _ in f)\n",
    "        self.num_samples = total_points\n",
    "        self.indices = np.arange(self.__len__())\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def _load_chunk(self, start):\n",
    "        points = []\n",
    "        labels = []\n",
    "        scenes = []\n",
    "\n",
    "        with open(self.points_file, 'r') as pf, open(self.encodings_file, 'r') as ef:\n",
    "            # Skip lines to start reading from the correct position\n",
    "            for _ in range(start*self.batch_size):\n",
    "                pf.readline()\n",
    "\n",
    "            for _ in range((start*self.batch_size)//self.points_per_scene):\n",
    "                for _ in range(self.resolution):\n",
    "                    ef.readline()\n",
    "                ef.readline()\n",
    "\n",
    "            for _ in range(self.batch_size):\n",
    "                point_line = pf.readline()\n",
    "                if not point_line.strip():\n",
    "                    break\n",
    "\n",
    "                components = point_line.strip().split()\n",
    "                points.append([float(x) for x in components[:4]])\n",
    "                labels.append(int(components[4]))\n",
    "            \n",
    "            l = []\n",
    "            if self.points_per_scene-(start*self.batch_size)%self.points_per_scene>=self.batch_size:\n",
    "                l.append(self.batch_size)\n",
    "            else:\n",
    "                l.append(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                remaining = self.batch_size-(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                for _ in range(remaining//self.points_per_scene):\n",
    "                    l.append(self.points_per_scene)\n",
    "                if (remaining%self.points_per_scene>0):\n",
    "                    l.append(remaining%self.points_per_scene)\n",
    "\n",
    "            not_available = False\n",
    "            for __ in l:\n",
    "                scene = []\n",
    "                for _ in range(self.resolution):\n",
    "                    scene_line = ef.readline()\n",
    "                    if not scene_line.strip():\n",
    "                        not_available = True\n",
    "                        break\n",
    "                    scene.append([float(x) for x in scene_line.strip().split()])\n",
    "                if not_available:\n",
    "                    break\n",
    "                ef.readline()\n",
    "                for _ in range(__):\n",
    "                    scenes.append(scene)\n",
    "\n",
    "        return np.array(points, dtype=float), np.array(labels, dtype=int), np.array(scenes, dtype=float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start = self.indices[idx]\n",
    "\n",
    "        if self.caching and start<int(self.__len__()*self.cache_factor) and self.cache[start]!=False:\n",
    "            (combined,labels) = self.cache[start]\n",
    "            return combined,labels\n",
    "        \n",
    "        # Load the chunk containing the required batch\n",
    "        points, labels, scenes = self._load_chunk(start)\n",
    "\n",
    "        combined = []\n",
    "        for i, scene in enumerate(scenes):\n",
    "            point_grid = np.zeros((self.resolution, self.resolution), dtype=np.float32)\n",
    "            p1_x, p1_y, p2_x, p2_y = points[i]\n",
    "            point_grid[min(int(p1_x * self.resolution), self.resolution-1),\n",
    "                       min(int(p1_y * self.resolution), self.resolution-1)] = 1.0\n",
    "            point_grid[min(int(p2_x * self.resolution), self.resolution-1),\n",
    "                       min(int(p2_y * self.resolution), self.resolution-1)] = 1.0\n",
    "\n",
    "            combined.append(np.stack([scene, point_grid], axis=-1))  # Shape: (resolution, resolution, 2)\n",
    "\n",
    "            if self.caching and start<int(self.__len__()*self.cache_factor):\n",
    "                self.cache[start] = (np.array(combined), np.array(labels, dtype=np.float32))\n",
    "\n",
    "        return np.array(combined), np.array(labels, dtype=np.float32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# points_file = \"../datasets/open_building_2D_cor_64x64.txt\"\n",
    "# encodings_file = \"../datasets/open_building_2D_encoding_64x64.txt\"\n",
    "points_file = \"../datasets/tt_10.txt\"\n",
    "encodings_file = \"../datasets/scenett_10.txt\"\n",
    "batch_size = 1024\n",
    "resolution = 32\n",
    "points_per_scene = 10\n",
    "\n",
    "\n",
    "dataset = TwoChannelVisibilityDataset(points_file, encodings_file, points_per_scene, resolution, batch_size,cache_factor=0.5)\n",
    "\n",
    "\n",
    "for data, batch_labels in dataset:\n",
    "    print(\"Batch Data Shape:\", data.shape)\n",
    "    print(\"Batch Labels Shape:\", batch_labels.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
