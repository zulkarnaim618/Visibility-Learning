{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/data001/test_coordinates.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 212\u001b[0m\n\u001b[0;32m    208\u001b[0m resolution \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m    209\u001b[0m points_per_scene \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m750\u001b[39m\n\u001b[1;32m--> 212\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTwoChannelVisibilityDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencodings_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_per_scene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, batch_labels \u001b[38;5;129;01min\u001b[39;00m dataset:\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mTwoChannelVisibilityDataset.__init__\u001b[1;34m(self, points_file, encodings_file, points_per_scene, resolution, batch_size, caching, cache_factor)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_factor \u001b[38;5;241m=\u001b[39m cache_factor\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_factor))]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaching:\n",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mTwoChannelVisibilityDataset._load_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoints_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m         total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m f)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m=\u001b[39m total_points\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/data001/test_coordinates.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class TwoChannelVisibilityDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, points_file, encodings_file, points_per_scene, resolution, batch_size=32, caching=True, cache_factor=1):\n",
    "        self.points_file = points_file\n",
    "        self.encodings_file = encodings_file\n",
    "        self.batch_size = batch_size\n",
    "        self.points_per_scene = points_per_scene\n",
    "        self.resolution = resolution\n",
    "        self.caching = caching\n",
    "        self.cache_factor = cache_factor\n",
    "        self.indices = None\n",
    "        self._load_indices()\n",
    "        self.cache = [False for i in range(int(self.__len__()*self.cache_factor))]\n",
    "        if self.caching:\n",
    "            self.load_cache()\n",
    "\n",
    "    def _load_indices(self):\n",
    "        with open(self.points_file, 'r') as f:\n",
    "            total_points = sum(1 for _ in f)\n",
    "        self.num_samples = total_points\n",
    "        self.indices = np.arange(self.__len__())\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "    \n",
    "    def load_cache(self):\n",
    "        # print(\"loading cache:\",len(self.cache))\n",
    "        last_scene = None\n",
    "        with open(self.points_file, 'r') as pf, open(self.encodings_file, 'r') as ef:\n",
    "            for start in range(int(self.__len__()*self.cache_factor)):\n",
    "                # print(\"start:\",start)\n",
    "                points = []\n",
    "                labels = []\n",
    "                scenes = []\n",
    "                l = []\n",
    "\n",
    "                \n",
    "                read_last_scene_again = False\n",
    "                \n",
    "                for _ in range(self.batch_size):\n",
    "                    point_line = pf.readline()\n",
    "                    if not point_line.strip():\n",
    "                        break\n",
    "\n",
    "                    components = point_line.strip().split()\n",
    "                    points.append([float(x) for x in components[:4]])\n",
    "                    labels.append(int(components[4]))\n",
    "                \n",
    "                if (start*self.batch_size)%self.points_per_scene>0:\n",
    "                    read_last_scene_again = True\n",
    "                    # print(\"Reading last scene again\")\n",
    "\n",
    "                if self.points_per_scene-(start*self.batch_size)%self.points_per_scene>=self.batch_size:\n",
    "                    l.append(self.batch_size)\n",
    "                else:\n",
    "                    l.append(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                    remaining = len(points)-(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                    for _ in range(remaining//self.points_per_scene):\n",
    "                        l.append(self.points_per_scene)\n",
    "                    if (remaining%self.points_per_scene>0):\n",
    "                        l.append(remaining%self.points_per_scene)\n",
    "\n",
    "                not_available = False\n",
    "\n",
    "                # print(start,(start*self.batch_size)%self.points_per_scene,read_last_scene_again,len(l),l)\n",
    "\n",
    "                for i in range(len(l)):\n",
    "                    __ = l[i]\n",
    "                    if i==0 and read_last_scene_again:\n",
    "                        scene = last_scene\n",
    "                    else:\n",
    "                        scene = []\n",
    "                        for _ in range(self.resolution):\n",
    "                            scene_line = ef.readline()\n",
    "                            if not scene_line.strip():\n",
    "                                not_available = True\n",
    "                                break\n",
    "                            scene.append([float(x) for x in scene_line.strip().split()])\n",
    "                        if not_available:\n",
    "                            break\n",
    "                        ef.readline()\n",
    "                    last_scene = scene\n",
    "                    # for _ in range(__):\n",
    "                    scenes.append(scene)\n",
    "\n",
    "                # combined = []\n",
    "                # for i, scene in enumerate(scenes):\n",
    "                #     point_grid = np.zeros((self.resolution, self.resolution), dtype=np.float32)\n",
    "                #     p1_x, p1_y, p2_x, p2_y = points[i]\n",
    "                #     point_grid[min(int(p1_x * self.resolution), self.resolution-1),\n",
    "                #             min(int(p1_y * self.resolution), self.resolution-1)] = 1.0\n",
    "                #     point_grid[min(int(p2_x * self.resolution), self.resolution-1),\n",
    "                #             min(int(p2_y * self.resolution), self.resolution-1)] = 1.0\n",
    "\n",
    "                #     combined.append(np.stack([scene, point_grid], axis=-1))  # Shape: (resolution, resolution, 2)\n",
    "\n",
    "                self.cache[start] = (np.array(points, dtype=float), np.array(labels, dtype=int), np.array(scenes, dtype=float), l)\n",
    "\n",
    "            print(\"finished loading cache\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def _load_chunk(self, start):\n",
    "        points = []\n",
    "        labels = []\n",
    "        scenes = []\n",
    "        l = []\n",
    "\n",
    "        with open(self.points_file, 'r') as pf, open(self.encodings_file, 'r') as ef:\n",
    "            # Skip lines to start reading from the correct position\n",
    "            for _ in range(start*self.batch_size):\n",
    "                pf.readline()\n",
    "\n",
    "            for _ in range((start*self.batch_size)//self.points_per_scene):\n",
    "                for _ in range(self.resolution):\n",
    "                    ef.readline()\n",
    "                ef.readline()\n",
    "\n",
    "            for _ in range(self.batch_size):\n",
    "                point_line = pf.readline()\n",
    "                if not point_line.strip():\n",
    "                    break\n",
    "\n",
    "                components = point_line.strip().split()\n",
    "                points.append([float(x) for x in components[:4]])\n",
    "                labels.append(int(components[4]))\n",
    "            \n",
    "            if self.points_per_scene-(start*self.batch_size)%self.points_per_scene>=self.batch_size:\n",
    "                l.append(self.batch_size)\n",
    "            else:\n",
    "                l.append(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                remaining = len(points)-(self.points_per_scene-(start*self.batch_size)%self.points_per_scene)\n",
    "                for _ in range(remaining//self.points_per_scene):\n",
    "                    l.append(self.points_per_scene)\n",
    "                if (remaining%self.points_per_scene>0):\n",
    "                    l.append(remaining%self.points_per_scene)\n",
    "\n",
    "            not_available = False\n",
    "            for __ in l:\n",
    "                scene = []\n",
    "                for _ in range(self.resolution):\n",
    "                    scene_line = ef.readline()\n",
    "                    if not scene_line.strip():\n",
    "                        not_available = True\n",
    "                        break\n",
    "                    scene.append([float(x) for x in scene_line.strip().split()])\n",
    "                if not_available:\n",
    "                    break\n",
    "                ef.readline()\n",
    "                # for _ in range(__):\n",
    "                scenes.append(scene)\n",
    "\n",
    "        return np.array(points, dtype=float), np.array(labels, dtype=int), np.array(scenes, dtype=float), l\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start = self.indices[idx]\n",
    "\n",
    "        if self.caching and start<int(self.__len__()*self.cache_factor) and self.cache[start]!=False:\n",
    "            points, labels, scenes, l = self.cache[start]\n",
    "            # print(\"reading from cache\",start)\n",
    "        else:\n",
    "            # Load the chunk containing the required batch\n",
    "            points, labels, scenes, l = self._load_chunk(start)\n",
    "            if self.caching and start<int(self.__len__()*self.cache_factor) and self.cache[start] == False:\n",
    "                self.cache[start] = (points, labels, scenes, l)\n",
    "\n",
    "        expanded_scenes = []\n",
    "        for i in range(len(l)):\n",
    "            for j in range(l[i]):\n",
    "                expanded_scenes.append(scenes[i])\n",
    "\n",
    "        expanded_scenes = np.array(expanded_scenes, dtype=float)\n",
    "\n",
    "        combined = []\n",
    "        for i, scene in enumerate(expanded_scenes):\n",
    "            point_grid = np.zeros((self.resolution, self.resolution), dtype=np.float32)\n",
    "            p1_x, p1_y, p2_x, p2_y = points[i]\n",
    "            point_grid[min(int(p1_x * self.resolution), self.resolution-1),\n",
    "                       min(int(p1_y * self.resolution), self.resolution-1)] = 1.0\n",
    "            point_grid[min(int(p2_x * self.resolution), self.resolution-1),\n",
    "                       min(int(p2_y * self.resolution), self.resolution-1)] = 1.0\n",
    "\n",
    "            combined.append(np.stack([scene, point_grid], axis=-1))  # Shape: (resolution, resolution, 2)\n",
    "\n",
    "        return np.array(combined), np.array(labels, dtype=np.float32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "\n",
    "# points_file = \"../datasets/open_building_2D_cor_64x64.txt\"\n",
    "# encodings_file = \"../datasets/open_building_2D_encoding_64x64.txt\"\n",
    "points_file = \"/kaggle/input/data001/test_coordinates.txt\"\n",
    "encodings_file = \"/kaggle/input/data001/test_encodings.txt\"\n",
    "batch_size = 10000\n",
    "resolution = 64\n",
    "points_per_scene = 750\n",
    "\n",
    "\n",
    "dataset = TwoChannelVisibilityDataset(points_file, encodings_file, points_per_scene, resolution, batch_size,cache_factor=1)\n",
    "\n",
    "k = 1\n",
    "for data, batch_labels in dataset:\n",
    "    print(\"k:\",k)\n",
    "    print(\"Batch Data Shape:\", data.shape)\n",
    "    print(\"Batch Labels Shape:\", batch_labels.shape)\n",
    "    k+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
